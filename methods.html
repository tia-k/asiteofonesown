<!DOCTYPE html>
<html>
    <head>
        <title>A (Short) Study in Sherlock - The Method</title>
        <link rel="stylesheet" type="text/css" href="styles.css" />
        <link rel="icon" type="image/x-icon" href="icons8-cube-24.png" />
    </head>
    <body>
        <div class="container">
            <div class="top">
                <h1>a (short) study in sherlock</h1>
                <p>A little website, for the purpose of a concise exercise in the digital humanities.</p>
                <p><a href="./">[index]</a> <b><a href="./methods">[methodology]</a></b> <a href="./python">[python code]</a></p>
            </div>
            <div class="main">
                <div class="authorsnote">
                    <p><i>Author's note: This website is a very rough and ready project, because it feels far more in the spirit of the digital humanities than a word document. It runs best in Chrome on a desktop or laptop PC. The author has made every effort to ensure the site runs, but, if it fails to, a version in a word document is provided.</i></p>
                </div>
                <div class="essay">
                    <h2>methodology and ethos.</h2>
                    <p>
                        To test my hypothesis that Arthur Conan Doyle developed a semantic and structural formula and applied it across the Sherlock Holmes series, both in the full-length novels and the short stories, I have undertaken four main methods, which I will explain here.<br><br>
                    </p>
                    <h3>1. Type-Token Analysis</h3>
                    <p>Type-token analysis refers to an attempt to measure the lexical variety of a text by comparing the number of unique words it contains (types) to the total amount of words it contains (tokens). The type-token ratio, abbreviated as TTR, the ratio of types to tokens. I have represented this as a number between zero and one, calculated by dividing the number of types by the number of tokens. This value can act as a proxy for lexical variety: a TTR of one is the most variety possible, a TTR of zero is the least variety possible. <br><br>
                    </p>
                    <h3>2. Sentiment Analysis</h3>
                    <p>
                        When I refer to "sentiment" here, it refers to how "positive" or "negative" the language of a text or section of a text is. I have performed sentiment analysis using the Python version of the open-source <a href="https://github.com/cjhutto/vaderSentiment" target="_blank">VADER sentiment analysis tool</a>. . VADER's database consists of individual words, rated as positive or negative on a scale from -4 to +4, as gathered through crowdsourcing. The sentiment analyser tool can apply these ratings to each word in a sentence and ultimately calculate a 'normalized, weighted, composite score' for the positive or negative sentiment of a pre-determined section on a scale of -1 (the most negative) to +1 (the most positive).  In my code, I have elected to do this analysis for each complete sentence in a text, and then take averages for each five-per cent of the text.<br><br>
                    </p>
                    <h3>3. Networks</h3>
                    <p>
                        The networks in this analysis' main body have been manually created by myself. Each character (node) is represented as having a connection (edge) to another character whenever there is an explicit exchange of dialogue, represented, recalled, or reported. This data, stored in CSVs, has been put into Stanford's <a href="https://hdlab.stanford.edu/palladio/" target="_blank">Palladio</a> tool to visualise connections. It was also run through <a href="https://networknavigator.jrladd.com/" target="_blank">Network Navigator</a>, though this data was not used. To get this project out promptly, I only created network data for the first five chapters of the first and last novels in the series and the first short story of each of the five short story collections.<br><br>
                    </p>
                    <h3>4. Natural Language Processing and Parts-of-Speech</h3>
                    <p>
                        Here, Natural Language Processing refers to computational recognition of parts-of-speech such as verbs, nouns, and proper names. For my reading, I have used <a href="https://spacy.io/" target="_blank">spaCy</a>to recognise these parts-of-speech. Using spaCy, I processed each text to gather the words associated with different parts of speech, which I then used other Python modules to count.<br><br>
                    </p>
                    <h3>Miscellaneous Notes</h3>
                    <p>
                       I have aimed to make my methods as easy to replicate as possible: scripts can be viewed in part on another page on this site, and the raw text for my corpus comes from the ever-useful <a href="https://www.gutenberg.org/">Project Gutenberg</a>. Though not all the data I have produced is viewable here or used in my analysis, all of it is available upon request. I have cleaned both the corpus data and my own produced data to some degree, but I fully acknowledge that my data has not been perfectly cleaned, mainly to get this project out on time. Nonetheless, I hope my data is clean enough and my code well explained enough that the vision for this project is apparent, even if I have yet to achieve it in the most complete and elegant manner on my own. 
                    </p>
                </div>
                <div class="footnotes">
                    <ol>
                        <li id="footnote_one">cjHutto, ‘VaderSentiment Documentation’, <i><a href="https://github.com/cjhutto/vaderSentiment" target="_blank">VaderSentiment - GitHub</a></i> [accessed 16 December 2023].<a href="#footnote_one_indic">^</a></li>
                    </ol>
                    <ul>
                        <li><a target="_blank" href="https://icons8.com/icon/98119/cube">Cube</a> icon by <a target="_blank" href="https://icons8.com">Icons8</a></li>
                    </ul>
                </div>
            </div>
        </div>
    </body>
</html>
