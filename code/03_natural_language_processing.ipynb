{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing for parts-of-speech with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Requirement already satisfied: spacy in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.26.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.5)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.1/12.8 MB 544.7 kB/s eta 0:00:24\n",
      "      --------------------------------------- 0.3/12.8 MB 1.7 MB/s eta 0:00:08\n",
      "     - -------------------------------------- 0.6/12.8 MB 3.2 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.1/12.8 MB 4.7 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 1.6/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 2.2/12.8 MB 7.0 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 3.1/12.8 MB 8.5 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.5/12.8 MB 8.9 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.2/12.8 MB 9.3 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 4.2/12.8 MB 9.3 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 6.3/12.8 MB 11.6 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 11.9 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.8/12.8 MB 11.9 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.7/12.8 MB 12.7 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.5/12.8 MB 13.0 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.5/12.8 MB 16.0 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 10.8/12.8 MB 16.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 11.1/12.8 MB 16.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 12.4/12.8 MB 17.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 16.8 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 16.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.5)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Installing spaCy\n",
    "!pip install -U spacy\n",
    "\n",
    "# Downloading spaCy's English-language model\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy # Used for natural language processing\n",
    "from spacy import displacy\n",
    "from collections import Counter # Used for counting the most/least common words/bigrams\n",
    "import pandas as pd # Used to shape data as tables and import that data into Excel\n",
    "import glob # Used for importing files matching a specific pattern\n",
    "from pathlib import Path # Used to interact more easily with the file system\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm') # Loads spaCy's English-language model and gives it the name 'nlp' to make it easier to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing functions for getting parts-of-speech data and bigram data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for working out the most common adjectives, nouns, pronouns and verbs in each text\n",
    "def top_speech_parts(filepath):\n",
    "    with open(filepath, encoding=\"utf-8\") as f: # Opens the .txt file (document) as variable f\n",
    "        document = nlp(f.read().lower()) # Reads the .txt file into the 'document' variable, applies NLP to it and then closes the file\n",
    "    adjs = [] # Creates an empty list that will store the adjective data\n",
    "    nouns = [] # Creates an empty list that will store the noun data\n",
    "    pronouns = [] # Creates an empty list that will store the pronoun data\n",
    "    verbs = [] # Creates an empty list that will store the verb data\n",
    "    \n",
    "    for token in document: # For every word in the document...\n",
    "        if token.pos_ == 'ADJ': # If it is an adjective...\n",
    "            adjs.append(token.text) # Record it to the adjective list\n",
    "        elif token.pos_ == 'NOUN': # If it is a noun...\n",
    "            nouns.append(token.text) # Record it to the noun list\n",
    "        elif token.pos_ == 'PRON': # If it is a pronoun...\n",
    "            pronouns.append(token.text) # Record it to the pronoun list\n",
    "        elif token.pos_ == 'VERB': # If it is a verb...\n",
    "            verbs.append(token.text) # Record it to the verb list\n",
    "        else: # If it is not any of these...\n",
    "            pass # Go on to the next word\n",
    "    \n",
    "    tokens = make_tokens(document) # Stores the value of what the make_tokens function created below returns\n",
    "\n",
    "    adjs_tally = Counter(adjs) # Counts how many times each adjective is used in a tuple with the word and its number of incidences\n",
    "    adjs_tally = adjs_tally.most_common() # Reorders this data from most common to least common\n",
    "\n",
    "    nouns_tally = Counter(nouns) # Counts how many times each noun is used in a tuple with the word and its number of incidences\n",
    "    nouns_tally = nouns_tally.most_common() # Reorders this data from most common to least common\n",
    "\n",
    "    pronouns_tally = Counter(pronouns) # Counts how many times each pronoun is used in a tuple with the word and its number of incidences\n",
    "    pronouns_tally = pronouns_tally.most_common() # Reorders this data from most common to least common\n",
    "\n",
    "    verbs_tally = Counter(verbs) # Counts how many times each verb is used in a tuple with the word and its number of incidences\n",
    "    verbs_tally = verbs_tally.most_common() # Reorders this data from most common to least common\n",
    "    \n",
    "    return adjs_tally, nouns_tally, pronouns_tally, verbs_tally, tokens # Returns the tallies of adjectives, nouns, pronouns and verbs, and the list of tokens in the text and their part-of-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for splitting a document into a list of words and their parts-of-speech tags\n",
    "def make_tokens(doc): # Takes in an already opened document\n",
    "    tokens_and_labels = [] # Creates an empty list that will store tokens and their associated part-of-speech type\n",
    "    \n",
    "    for token in doc: # For every word in the document...\n",
    "        if token.is_alpha: # If it consists of alphabetic characters...\n",
    "            tokens_and_labels.append((token.text, token.pos_)) # Add a tuple containing it and it's part-of-speech tag to the list\n",
    "    \n",
    "    return tokens_and_labels # Returns this list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for getting ngrams (groups of consecutive words)\n",
    "def get_ngrams(word_list,number_consecutive_words): # Takes in a pre-made list of tokens and their part-of-speech tags, and the desired length of ngram\n",
    "    ngrams = [] # Creates an empty list that will be used to store ngrams (groups of a certain number of words)\n",
    "    adjusted_word_list_len = len(word_list)-(number_consecutive_words-1) # Works out the number up to which we can find ngrams of the stated length\n",
    "    for i in range(adjusted_word_list_len): # For each number word in this document...\n",
    "        ngram = word_list[i:i+number_consecutive_words] # Save the word and the word after it to the variable ngram\n",
    "        ngrams.append(ngram) # Add this ngram to the list of ngrams\n",
    "    \n",
    "    return ngrams # Returns the list of ngrams in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for seeing which words appear close to a searched for word\n",
    "def get_neighbour_words(keyword,bigrams,pos_label='None'): # Takes in a keyword we're looking for, a list of ngrams (bigrams in our case), and the part-of-speech tag we're looking for, setting it as 'None' by default\n",
    "    neighbour_words = [] # Creates an empty list that will store words found next to the searched for keyword\n",
    "    keyword = keyword.lower() # Makes the keyword lowercase - all words will be made lowercase so case-sensitivity isn't an issue\n",
    "    \n",
    "    for bigram in bigrams: # For every ngram in the list of ngrams (in our case, bigrams - this is accurate but also a variable name held over from a prior version of the code)\n",
    "        words = [word.lower() for word, label in bigram] # Sets the words to lowercase in each bigram\n",
    "        if keyword in words: # If the keyword is found in the bigram...\n",
    "            for word, label in bigram: # For each word and associated label...\n",
    "                if word.lower() != keyword: # If the word isn't the keyword...\n",
    "                    if label == pos_label or pos_label == None: # And it is the part-of-speech we're looking for, or we aren't targeting a part of speech\n",
    "                        neighbour_words.append(word.lower()) # Add this to the list of words used around the keyword (neighbour words)\n",
    "    \n",
    "    return Counter(neighbour_words).most_common() # Returns the list of neighbour words a) counted and put into tuples and b) ordered from most to least common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of corpus documents and their titles\n",
    "directory_path = 'resources/doyle_holmes_comb/' # Sets directory path to the combined Holmes corupus\n",
    "document_files = glob.glob(f'{directory_path}/*.txt') # Creates a list with the name of each .txt file (document) in the directory\n",
    "document_files = [slashes.replace('\\\\', '/') for slashes in document_files] # Replaces double backslashes with forward slashes so Python can find these files\n",
    "document_titles = [Path(document).stem for document in document_files] # Creates a list of document titles by taking the file list and removing the trailing '.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1887_Study_in_Scarlet\n",
      "1890_Sign_of_the_Four\n",
      "1892_Adventures_of_Sherlock_Holmes\n",
      "1894_Memoirs_of_Sherlock_Holmes\n",
      "1902_Hound_of_the_Baskervilles\n",
      "1905_Return_of_Sherlock_Holmes\n",
      "1915_Valley_of_Fear\n",
      "1917_His_Last_Bow\n",
      "1927_Case-book_of_Sherlock_Holmes\n"
     ]
    }
   ],
   "source": [
    "for document_filepath, document_title in zip(document_files, document_titles): # For every filepath and its associated document title...\n",
    "    # Retrieve part-of-speech data\n",
    "    adjs,nouns,pronouns,verbs,tokens = top_speech_parts(document_filepath) # Apply the top_speech_parts function to the document and store the results\n",
    "\n",
    "    # Convert part-of-speech data to dataframes\n",
    "    adjs_df = pd.DataFrame(adjs, columns=['adjective', 'count']) # Convert the adjective data into a Pandas dataframe (a table, essentially)\n",
    "    nouns_df = pd.DataFrame(nouns, columns=['noun', 'count']) # Convert the noun data into a Pandas dataframe \n",
    "    pronouns_df = pd.DataFrame(pronouns, columns=['pronoun', 'count']) # Convert the pronoun data into a Pandas dataframe \n",
    "    verbs_df = pd.DataFrame(verbs, columns=['verb', 'count']) # Convert the verb data into a Pandas dataframe\n",
    "\n",
    "    # Write these dataframes to an Excel file\n",
    "    with pd.ExcelWriter(f'natural_lang/expanded/parts_of_speech/{document_title.lower()}_parts_of_speech.xlsx', engine='xlsxwriter') as writer: # Open an Excel file in the listed folder with the name of the document and '_parts_of_speech' and ...\n",
    "        adjs_df.to_excel(writer, sheet_name=f'adjectives') # Writes the adjective data to a sheet called 'adjectives'\n",
    "        nouns_df.to_excel(writer, sheet_name=f'nouns') # Writes the noun data to a sheet called 'nouns'\n",
    "        pronouns_df.to_excel(writer, sheet_name=f'pronouns') # Writes the pronoun data to a sheet called 'pronouns'\n",
    "        verbs_df.to_excel(writer, sheet_name=f'verbs') # Writes the verb data to a sheet called 'verbs'\n",
    "        # The file then closes\n",
    "    \n",
    "    # Retrieve bigrams for Sherlock Holmes\n",
    "    ngrams = get_ngrams(tokens,2) # Get bigrams from the list of tokens we made earlier\n",
    "    sherlock_adjs = get_neighbour_words('Sherlock',ngrams,pos_label='ADJ') # Get the ordered list of adjectives used around 'Sherlock' by running get_neighbour_words\n",
    "    sherlock_verbs = get_neighbour_words('Sherlock',ngrams,pos_label='VERB') # Get the ordered list of verbs used around 'Sherlock' by running get_neighbour_words\n",
    "    holmes_adjs = get_neighbour_words('Holmes',ngrams,pos_label='ADJ') # Get the ordered list of adjectives used around 'Holmes' by running get_neighbour_words\n",
    "    holmes_verbs = get_neighbour_words('Holmes',ngrams,pos_label='VERB') # Get the ordered list of verbs used around 'Holmes' by running get_neighbour_words\n",
    "\n",
    "    # Convert bigram data to dataframes\n",
    "    sherlock_adjs_df = pd.DataFrame(sherlock_adjs, columns=['adjective', 'count']) # Convert the 'Sherlock' adjectives to a Pandas dataframe\n",
    "    sherlock_verbs_df = pd.DataFrame(sherlock_verbs, columns=['verb', 'count']) # Convert the 'Sherlock' verbs to a Pandas dataframe\n",
    "    holmes_adjs_df = pd.DataFrame(holmes_adjs, columns=['adjective', 'count']) # Convert the 'Holmes' adjectives to a Pandas dataframe\n",
    "    holmes_verbs_df = pd.DataFrame(holmes_verbs, columns=['verb', 'count']) # Convert the 'Holmes' verbs to a Pandas dataframe\n",
    "\n",
    "    # Write these dataframes to an Excel file\n",
    "    with pd.ExcelWriter(f'natural_lang/expanded/bigrams/{document_title.lower()}_bigrams.xlsx', engine='xlsxwriter') as writer: # This opens an Excel file in the listed folder with the name of the document and '_bigrams' and ...\n",
    "        sherlock_adjs_df.to_excel(writer, sheet_name=f'sherlock_adj') # Writes the 'Sherlock' adjectives to a sheet called 'sherlock_adj'\n",
    "        sherlock_verbs_df.to_excel(writer, sheet_name=f'sherlock_verb') # Writes the 'Sherlock' verbs to a sheet called 'sherlock_verb'\n",
    "        holmes_adjs_df.to_excel(writer, sheet_name=f'holmes_adj') # Writes the 'Holmes' adjectives to a sheet called 'holmes_adj'\n",
    "        holmes_verbs_df.to_excel(writer, sheet_name=f'holmes_verb') # Writes the 'Holmes' verbs to a sheet called 'holmes_verb'\n",
    "        # The file then closes\n",
    "\n",
    "    print(document_title) # This prints the document title when it's done doing the above to let the user know it has been processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update / progress log\n",
    "- I spent a significant amount of time troubleshooting only to find out I had not included an underscore in .pos_ (13.12.2023, 14:14)\n",
    "- Made the decision to take the simplest route and make each document its own Excel document which I will then combine manually (13.12.2023, 14:32)\n",
    "- Updated the parts-of-speech section to convert all words to lowercase so I don't get double-counts for the same word (13.12.2023, 20:11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve bigrams for Sherlock Holmes\n",
    "    ngrams = get_ngrams(tokens,2) # Get bigrams from the list of tokens we made earlier\n",
    "    sherlock_adjs = get_neighbour_words('Sherlock',ngrams,pos_label='ADJ') # Get the ordered list of adjectives used around 'Sherlock' by running get_neighbour_words\n",
    "    sherlock_verbs = get_neighbour_words('Sherlock',ngrams,pos_label='VERB') # Get the ordered list of verbs used around 'Sherlock' by running get_neighbour_words\n",
    "    holmes_adjs = get_neighbour_words('Holmes',ngrams,pos_label='ADJ') # Get the ordered list of adjectives used around 'Holmes' by running get_neighbour_words\n",
    "    holmes_verbs = get_neighbour_words('Holmes',ngrams,pos_label='VERB') # Get the ordered list of verbs used around 'Holmes' by running get_neighbour_words\n",
    "\n",
    "    # Convert bigram data to dataframes\n",
    "    sherlock_adjs_df = pd.DataFrame(sherlock_adjs, columns=['adjective', 'count']) # Convert the 'Sherlock' adjectives to a Pandas dataframe\n",
    "    sherlock_verbs_df = pd.DataFrame(sherlock_verbs, columns=['verb', 'count']) # Convert the 'Sherlock' verbs to a Pandas dataframe\n",
    "    holmes_adjs_df = pd.DataFrame(holmes_adjs, columns=['adjective', 'count']) # Convert the 'Holmes' adjectives to a Pandas dataframe\n",
    "    holmes_verbs_df = pd.DataFrame(holmes_verbs, columns=['verb', 'count']) # Convert the 'Holmes' verbs to a Pandas dataframe\n",
    "\n",
    "    # Write these dataframes to an Excel file\n",
    "    with pd.ExcelWriter(f'natural_lang/expanded/bigrams/{document_title.lower()}_bigrams.xlsx', engine='xlsxwriter') as writer: # This opens an Excel file in the listed folder with the name of the document and '_bigrams' and ...\n",
    "        sherlock_adjs_df.to_excel(writer, sheet_name=f'sherlock_adj') # Writes the 'Sherlock' adjectives to a sheet called 'sherlock_adj'\n",
    "        sherlock_verbs_df.to_excel(writer, sheet_name=f'sherlock_verb') # Writes the 'Sherlock' verbs to a sheet called 'sherlock_verb'\n",
    "        holmes_adjs_df.to_excel(writer, sheet_name=f'holmes_adj') # Writes the 'Holmes' adjectives to a sheet called 'holmes_adj'\n",
    "        holmes_verbs_df.to_excel(writer, sheet_name=f'holmes_verb') # Writes the 'Holmes' verbs to a sheet called 'holmes_verb'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
