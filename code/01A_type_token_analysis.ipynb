{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type-Token Analysis without split hyphenations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Separated short stories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob # Used for importing files matching a specific pattern\n",
    "from pathlib import Path # Used to interact more easily with the file system\n",
    "import pandas as pd # Used to shape data as tables and import that data into Excel\n",
    "import string # Used to have the ascii list of characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and setting up functions for cleaning up and processing passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of corpus documents and their titles\n",
    "directory_path = 'resources/doyle_holmes_sep/' # Sets directory path to the separated Holmes corpus\n",
    "document_files = glob.glob(f'{directory_path}/*.txt') # Creates a list with the name of each .txt file (document) in the directory\n",
    "document_files = [slashes.replace('\\\\', '/') for slashes in document_files] # Replaces double backslashes with forward slashes so Python can find these files\n",
    "document_titles = [Path(document).stem for document in document_files] # Creates a list of document titles by taking the file list and removing the trailing '.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to clean the words for each document and setting up a stopwords lists\n",
    "acceptable_characters = list(string.ascii_lowercase) + list(string.ascii_uppercase) + list(string.digits) # Creates a list of acceptable characters (letters and numbers)\n",
    "acceptable_characters.append('-') # Adds a hyphen onto this list\n",
    "\n",
    "stopwords = open('resources/stopwords.txt', encoding='utf-8').read() # Opens the stopwords list and reads it into the variable stopwords\n",
    "stopwords = stopwords.split(\",\") # Splits this list of stopwords into a Python list\n",
    "\n",
    "def clean_words(word):\n",
    "    for char in word: # For every letter in a word...\n",
    "        if char not in acceptable_characters: # If it is not a hyphen, letter or number...\n",
    "            word = word.replace(char, '') # This removes it\n",
    "    return word.lower() # Returns the word as lowercase with 'unacceptable' characters removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to do the type-token analysis for each text\n",
    "def type_token(filepath, title):\n",
    "    with open(f'{filepath}', encoding='utf8') as f: # Opens the .txt file as variable f\n",
    "        full_passage = f.read().lower() # Reads the .txt file into a string and then closes the file\n",
    "    full_passage = full_passage.replace('\\n', ' ') # Replaces new-line symbols with spaces\n",
    "    full_passage = full_passage.split() # Splits this string into a list of words\n",
    "    for word in full_passage: # For each word in the list of words...\n",
    "        word = clean_words(word) # This removes any remaining punctuation except for hyphens and makes it lowercase\n",
    "    tokens = len(full_passage) # Counts the amount of words in the list of all the document's words - this is the number of 'tokens' in the document\n",
    "    passage = [word for word in full_passage if word not in stopwords] # Makes a list of words with the stopwords removed (adds all the passage's words to a new list unless they are stopwords)\n",
    "\n",
    "    unique_terms = [passage[0]] # Creates a list of unique terms, starting with the passage's first non-stopword word\n",
    "    for word in passage: # For each word in the passage...\n",
    "        if word not in unique_terms: # If it hasn't already been recorded as a unique word...\n",
    "            unique_terms.append(word) # This adds the word to the list of unique words\n",
    "    types = len(unique_terms) # Counts the amount of words in the list of the document's unique words - this is the number of 'types'\n",
    "    ttr = round(types/tokens, 5) # Calculates the type-token ratio (number of types divided by number of tokens) and rounds this to five decimal places\n",
    "    passage_date = title[0:4] # Creates a value for the document's year (the first four characters of the file's name)\n",
    "    passage_month = title[5:7] # Creates a value for the document's month (the fifth and sixth characters of the file's name)\n",
    "    if passage_month[0] == '0': # If the document's month has a trailing zero\n",
    "        passage_month.replace('0', '') # This removes it\n",
    "    passage_month_n = int(passage_month) # And this converts it to an integer to do some maths\n",
    "    passage_month_n = passage_month_n - 1 # And this subtracts one for the purpose of some maths\n",
    "\n",
    "    # The following maths works by estimating one month into the year being 0.08333, so, if the year is 2023, January is 2023.000000 (so the month needs 1 subtracted), February is 2023.08333 and so on; the purpose of all this is to make visualisations in Excel easier\n",
    "    passage_month_calc = round(passage_month_n * 0.08333, 5) # Multiplies the month by 0.08333\n",
    "    passage_month = str(passage_month_calc) # Converts this float value back to a string\n",
    "    passage_month = passage_month[1:] # Removes the starting zero\n",
    "    passage_date = passage_date + passage_month # Adds this decimal onto the end of the year string\n",
    "    document_type_token = {'text': title, 'date': passage_date, 'TTR': ttr, 'types': types,'tokens': tokens} # Creates a dictionary containing the document's title, number of types, number of tokens, and type-token ratio (TTR)\n",
    "    return document_type_token # Returns this list to be used by the wider loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1887_12_Study_in_Scarlet\n",
      "1890_02_Sign_of_the_Four\n",
      "1891_06_Scandal_in_Bohemia\n",
      "1891_08_Red-Headed_League\n",
      "1891_09_Case_of_Identity\n",
      "1891_10_Boscome_Valley_Mystery\n",
      "1891_11_Five_Orange_Pips\n",
      "1891_12_Man_with_the_Twisted_Lip\n",
      "1892_01_Blue_Carbuncle\n",
      "1892_02_Speckled_Band\n",
      "1892_03_Engineers_Thumb\n",
      "1892_04_Noble_Bachelor\n",
      "1892_05_Beryl_Coronet\n",
      "1892_06_Copper_Beeches\n",
      "1892_12_Silver_Blaze\n",
      "1893_01_Cardboard_Box\n",
      "1893_02_Yellow_Face\n",
      "1893_03_Stockbrokers_Clerk\n",
      "1893_04_Gloria_Scott\n",
      "1893_05_Musgrave_Ritual\n",
      "1893_06_Reigate_Squire\n",
      "1893_07_Crooked_Man\n",
      "1893_08_Resident_Patient\n",
      "1893_09_Greek_Interpreter\n",
      "1893_11_Naval_Treaty\n",
      "1893_12_The_Final_Problem\n",
      "1902_03_Hound_of_the_Baskervilles\n",
      "1903_10_Empty_House\n",
      "1903_11_Norwood_Builder\n",
      "1903_12_Dancing_Men\n",
      "1904_01_Solitary_Cyclist\n",
      "1904_02_Priory_School\n",
      "1904_03_Black_Peter\n",
      "1904_04_Charles_Augustus_Milverton\n",
      "1904_05_Six_Napoleons\n",
      "1904_06_Three_Students\n",
      "1904_07_Golden_Pince-Nez\n",
      "1904_08_Missing_Three-Quarter\n",
      "1904_09_Abbey_Grange\n",
      "1904_12_Second_Stain\n",
      "1908_08_Wisteria_Lodge\n",
      "1908_12_Bruce-Partington_Plans\n",
      "1910_12_Devils_Foot\n",
      "1911_04_Red_Circle\n",
      "1911_12_Lady_Frances_Carfax\n",
      "1913_11_Dying_Detective\n",
      "1915_02_Valley_of_Fear\n",
      "1917_09_His_Last_Bow\n",
      "1921_10_Mazarin_Stone\n",
      "1922_03_Problem_of_Thor_Bridge\n",
      "1923_03_Creeping_Man\n",
      "1924_01_Sussex_Vampire\n",
      "1924_09_Three_Garridebs\n",
      "1924_11_Illustrious_Client\n",
      "1926_10_Blanched_Soldier\n",
      "1926_10_Three_Gables\n",
      "1926_11_Lions_Mane\n",
      "1926_12_Retired_Colourman\n",
      "1927_01_Veiled_Lodger\n",
      "1927_03_Shoscombe_Old_Place\n"
     ]
    }
   ],
   "source": [
    "# Running the type_token function on all the documents\n",
    "type_token_data = [] # Creates an empty array which will be used to hold the type-token data\n",
    "\n",
    "for document_filepath, document_title in zip(document_files, document_titles): # For every filepath and its associated document title...\n",
    "    type_token_data.append(type_token(document_filepath, document_title)) # This adds its type-token data to the array of all type-token data\n",
    "    print(document_title) # This prints the document title when it's done doing the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing this data to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlsxwriter in c:\\users\\tia work\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.1.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install xlsxwriter \n",
    "# Installs xlsxwriter so it can be used\n",
    "import xlsxwriter # Used as an engine for writing to Excel .xlsx files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writes the type-token data to a Pandas dataframe and then writes this to an Excel file, which we can then use for visualisations\n",
    "type_token_df = pd.DataFrame(type_token_data) # Makes the type-token data into a Pandas dataframe (a table, essentially)\n",
    "\n",
    "with pd.ExcelWriter('holmes_type_token_sep.xlsx', engine='xlsxwriter') as writer: # Opens an Excel spreadsheet with the above name and...\n",
    "    type_token_df.to_excel(writer, sheet_name=f'type_token') # Writes the type-token dataframe to it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update log\n",
    "- Decided to add a date to each document to facilitate plotting by date (12.12.2023, 14:31)\n",
    "- Fixed an issue with files not being closed (12.12.2023, 18:06)\n",
    "- Managed to get the date in a format that should cooperate with Excel (12.12.2023, 20:01)\n",
    "- Realised I had not run clean_words and thus hadn't made words lowercase and fixed this. (18.12.2023, 10:54)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "- 'Passages' rather than 'documents' or 'texts' in variable names is a holdover from an earlier version, which I have allowed to remain as is"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
