{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using vaderSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Tia\n",
      "[nltk_data]     Work\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer # Used to analyse the sentiment of sentences - this function is based on a database of words with positive/neutral/negative sentiment as rated by sample groups\n",
    "import glob # Used for importing files matching a specific pattern\n",
    "from pathlib import Path #Used to interact more easily with the file system\n",
    "import nltk # Used for tokenizing texts into sentences\n",
    "nltk.download('punkt') # The specific module needed for this sentence tokenizing\n",
    "import pandas as pd # Used to shape data as tables and import that data into Excel\n",
    "from statistics import mean # Used to calculate a mean without having to do the maths manually\n",
    "\n",
    "sentimentAnalyser = SentimentIntensityAnalyzer() # Converts this function to a British spelling and makes it easier to call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the sentiment analysis function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to do the sentiment analysis for each 5% of the text\n",
    "def find_sentiment(filepath,title):\n",
    "    with open(f'{filepath}', encoding='utf8') as f: # Opens the .txt file as variable f\n",
    "        text = f.read() # Reads the .txt file into a string and then closes the file\n",
    "    text = text.replace('\\n', ' ') # Replaces new-line symbols with spaces\n",
    "    text_sentences = nltk.sent_tokenize(text) # Splits the text up into a list of sentences using a NLTK (Natural Language Toolkit) method\n",
    "    \n",
    "    no_sentences = len(text_sentences) # Finds the number of sentences by getting the length of the list of sentences\n",
    "    start_pos = 0 # Creates a starting position for sentence number, beginning at zero (counting starts at zero in Python)\n",
    "    five_percent = round(no_sentences/20) # Calculates how many sentences are equivalent to five percent of the whole document\n",
    "\n",
    "    section_scores = {'text':f'{title}',} # Creates an empty dictionary which will go on to contain the sentiment score for each five percent, and records the document's name as its first key/value pair\n",
    "\n",
    "    for i in range(1, 21): # For each five percent...\n",
    "        total_scores = [] # This makes an empty list which will store the scores for each sentence\n",
    "        for sentence in text_sentences[start_pos:start_pos+five_percent]: # For every sentence in this five percent...\n",
    "            scores = sentimentAnalyser.polarity_scores(sentence) # This works out its scores\n",
    "            total_scores.append(scores['compound']) #This adds its overall score to the list of scores\n",
    "        \n",
    "        if len(total_scores) != 0: # If there are sentences in the list of scores for this five percent...\n",
    "            scores_count = mean(total_scores) # This works out the average of this scores and stores it\n",
    "        else: # If there aren't...\n",
    "            scores_count = 0 # This sets the overall score for that section to zero\n",
    "\n",
    "        section_scores[i*5] = float(scores_count) # Makes a new key/value pair in the dictionary, with the key of which percent it is, and the average sentiment value\n",
    "        start_pos += five_percent # Increases the starting position to after this five percent, so we work with the next five percent of the document\n",
    "\n",
    "    return section_scores # Returns the dictionary of a document's sentiment scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1887_12_Study_in_Scarlet\n",
      "1890_02_Sign_of_the_Four\n",
      "1891_06_Scandal_in_Bohemia\n",
      "1891_08_Red-Headed_League\n",
      "1891_09_Case_of_Identity\n",
      "1891_10_Boscome_Valley_Mystery\n",
      "1891_11_Five_Orange_Pips\n",
      "1891_12_Man_with_the_Twisted_Lip\n",
      "1892_01_Blue_Carbuncle\n",
      "1892_02_Speckled_Band\n",
      "1892_03_Engineers_Thumb\n",
      "1892_04_Noble_Bachelor\n",
      "1892_05_Beryl_Coronet\n",
      "1892_06_Copper_Beeches\n",
      "1892_12_Silver_Blaze\n",
      "1893_01_Cardboard_Box\n",
      "1893_02_Yellow_Face\n",
      "1893_03_Stockbrokers_Clerk\n",
      "1893_04_Gloria_Scott\n",
      "1893_05_Musgrave_Ritual\n",
      "1893_06_Reigate_Squire\n",
      "1893_07_Crooked_Man\n",
      "1893_08_Resident_Patient\n",
      "1893_09_Greek_Interpreter\n",
      "1893_11_Naval_Treaty\n",
      "1893_12_The_Final_Problem\n",
      "1902_03_Hound_of_the_Baskervilles\n",
      "1903_10_Empty_House\n",
      "1903_11_Norwood_Builder\n",
      "1903_12_Dancing_Men\n",
      "1904_01_Solitary_Cyclist\n",
      "1904_02_Priory_School\n",
      "1904_03_Black_Peter\n",
      "1904_04_Charles_Augustus_Milverton\n",
      "1904_05_Six_Napoleons\n",
      "1904_06_Three_Students\n",
      "1904_07_Golden_Pince-Nez\n",
      "1904_08_Missing_Three-Quarter\n",
      "1904_09_Abbey_Grange\n",
      "1904_12_Second_Stain\n",
      "1908_08_Wisteria_Lodge\n",
      "1908_12_Bruce-Partington_Plans\n",
      "1910_12_Devils_Foot\n",
      "1911_04_Red_Circle\n",
      "1911_12_Lady_Frances_Carfax\n",
      "1913_11_Dying_Detective\n",
      "1915_02_Valley_of_Fear\n",
      "1917_09_His_Last_Bow\n",
      "1921_10_Mazarin_Stone\n",
      "1922_03_Problem_of_Thor_Bridge\n",
      "1923_03_Creeping_Man\n",
      "1924_01_Sussex_Vampire\n",
      "1924_09_Three_Garridebs\n",
      "1924_11_Illustrious_Client\n",
      "1926_10_Blanched_Soldier\n",
      "1926_10_Three_Gables\n",
      "1926_11_Lions_Mane\n",
      "1926_12_Retired_Colourman\n",
      "1927_01_Veiled_Lodger\n",
      "1927_03_Shoscombe_Old_Place\n"
     ]
    }
   ],
   "source": [
    "# Setting up the directory\n",
    "directory_path = 'resources/doyle_holmes_sep' # Sets the directory path to the separated Holmes corpus\n",
    "document_files = glob.glob(f'{directory_path}/*.txt') # Creates a list with the name of each .txt file (document) in the directory\n",
    "document_files = [slashes.replace('\\\\', '/') for slashes in document_files] # Replaces double backslashes with forward slashes so Python can find these files\n",
    "document_titles = [Path(document).stem for document in document_files] # Creates a list of document titles by taking the file list and removing the trailing '.txt'\n",
    "\n",
    "# Running the find_sentiment function on all documents and recording the results\n",
    "percentage_data = [] # Creates an empty array which will be used to hold the sentiment data\n",
    "for document_filepath, document_title in zip(document_files, document_titles): # For every filepath and its associated document title...\n",
    "    percentage_data.append(find_sentiment(document_filepath,document_title)) # This applies the find_sentiment function on the document and adds the returned data to the array of all the sentiment data data\n",
    "    print(document_title) # This prints the document title when it's done doing the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing this data to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xlsxwriter\n",
      "  Obtaining dependency information for xlsxwriter from https://files.pythonhosted.org/packages/f7/3e/05ba2194cd5073602422859c949a4f21310a3c49bf8dccde9e03d4522b11/XlsxWriter-3.1.9-py3-none-any.whl.metadata\n",
      "  Downloading XlsxWriter-3.1.9-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading XlsxWriter-3.1.9-py3-none-any.whl (154 kB)\n",
      "   ---------------------------------------- 0.0/154.8 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/154.8 kB ? eta -:--:--\n",
      "   --------------- ----------------------- 61.4/154.8 kB 812.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 154.8/154.8 kB 1.5 MB/s eta 0:00:00\n",
      "Installing collected packages: xlsxwriter\n",
      "Successfully installed xlsxwriter-3.1.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Installs xlsxwriter so it can be used\n",
    "!pip install xlsxwriter\n",
    "import xlsxwriter # Used as an engine for writing to Excel .xlsx files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writes the sentiment data to a Pandas dataframe and then writes this to an Excel file, which we can then use for visualisations\n",
    "percentage_df = pd.DataFrame(percentage_data) # Makes the sentiment data into a Pandas dataframe (a table, essentially)\n",
    "with pd.ExcelWriter('holmes_sentiment_analysis.xlsx', engine='xlsxwriter') as writer: # Opens an Excel spreadsheet with the above name and...\n",
    "    percentage_df.to_excel(writer, sheet_name=f'sentiment_analysis') # Writes the sentiment dataframe to it, then closes it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
