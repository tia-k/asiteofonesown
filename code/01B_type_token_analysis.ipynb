{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type-Token Analysis without split hyphenations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Short stories as collections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob # Used for importing files matching a specific pattern\n",
    "from pathlib import Path # Used to interact more easily with the file system\n",
    "import pandas as pd # Used to shape data as tables and import that data into excel\n",
    "import string # Used to have the ascii list of characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and setting up functions for cleaning up and processing passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of corpus documents and their titles\n",
    "directory_path = 'resources/doyle_holmes_comb/' # Sets directory path to the combined Holmes corupus\n",
    "document_files = glob.glob(f'{directory_path}/*.txt') # Creates a list with the name of each .txt file (document) in the directory\n",
    "document_files = [slashes.replace('\\\\', '/') for slashes in document_files] # Replaces double backslashes with forward slashes so Python can find these files\n",
    "document_titles = [Path(document).stem for document in document_files] # Creates a list of document titles by taking the file list and removing the trailing '.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to clean the words for each document and setting up a stopwords lists\n",
    "acceptable_characters = list(string.ascii_lowercase) + list(string.ascii_uppercase) + list(string.digits) # Creates a list of acceptable characters (letters and numbers)\n",
    "acceptable_characters.append('-') # Adds a hyphen onto this list\n",
    "\n",
    "stopwords = open('resources/stopwords.txt', encoding='utf-8').read() # Opens the stopwords list and reads it into the variable stopwords\n",
    "stopwords = stopwords.split(\",\") # Splits this list of stopwords into a Python list\n",
    "\n",
    "def clean_words(word):\n",
    "    for char in word: # For every letter in a word...\n",
    "        if char not in acceptable_characters: # If it is not a hyphen, letter or number...\n",
    "            word = word.replace(char, '') # This removes it\n",
    "    return word.lower() # Returns the word with 'unacceptable' characters removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to do the type-token analysis for each text\n",
    "def type_token(filepath, title):\n",
    "    with open(f'{filepath}', encoding='utf8') as f: # Opens the .txt file as variable f\n",
    "        full_passage = f.read() # Reads the .txt file into a string and then closes the file\n",
    "    full_passage = full_passage.replace('\\n', ' ') # Replaces new-line symbols with spaces\n",
    "    full_passage = full_passage.split() # Splits this string into a list of words\n",
    "    tokens = len(full_passage) # Counts the amount of words in the list of all the document's words - this is the number of 'tokens' in the document\n",
    "    passage = [word for word in full_passage if word not in stopwords] # Makes a list of words with the stopwords removed (adds all the passage's words to a new list unless they are stopwords)\n",
    "\n",
    "    unique_terms = [passage[0]] # Creates a list of unique terms, starting with the passage's first non-stopword word\n",
    "    for word in passage: # For each word in the passage...\n",
    "        if word not in unique_terms: # If it hasn't already been recorded as a unique word...\n",
    "            unique_terms.append(word) # This adds the word to the list of unique words\n",
    "    types = len(unique_terms) # Counts the amount of words in the list of the document's unique words - this is the number of 'types'\n",
    "    ttr = round(types/tokens, 5) # Calculates the type-token ratio (number of types divided by number of tokens) and rounds this to five decimal places\n",
    "    passage_year = title[0:4] # Creates a value for the document's year (the first four characters of the file's name)\n",
    "\n",
    "    document_type_token = {'text': title, 'year': passage_year, 'TTR': ttr, 'types': types,'tokens': tokens} # Creates a dictionary containing the document's title, number of types, number of tokens, and type-token ratio (TTR)\n",
    "    return document_type_token # Returns this list to be used by the wider loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1887_Study_in_Scarlet\n",
      "1890_Sign_of_the_Four\n",
      "1892_Adventures_of_Sherlock_Holmes\n",
      "1894_Memoirs_of_Sherlock_Holmes\n",
      "1902_Hound_of_the_Baskervilles\n",
      "1905_Return_of_Sherlock_Holmes\n",
      "1915_Valley_of_Fear\n",
      "1917_His_Last_Bow\n",
      "1927_Case-book_of_Sherlock_Holmes\n"
     ]
    }
   ],
   "source": [
    "# Running the type_token function on all the documents\n",
    "type_token_data = [] # Creates an empty array which will be used to hold the type-token data\n",
    "\n",
    "for document_filepath, document_title in zip(document_files, document_titles): # For every filepath and its associated document title...\n",
    "    type_token_data.append(type_token(document_filepath, document_title)) # This adds its type-token data to the array of all type-token data\n",
    "    print(document_title) # This prints the document title when it's done doing the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing this data to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install xlsxwriter # Installs xlsxwriter so it can be used\n",
    "import xlsxwriter # Used as an engine for writing to Excel .xlsx files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writes the type-token data to a Pandas dataframe and then writes this to an Excel file, which we can then use for visualisations\n",
    "type_token_df = pd.DataFrame(type_token_data) # Makes the type-token data into a Pandas dataframe (a table, essentially)\n",
    "\n",
    "with pd.ExcelWriter('holmes_type_token_comb.xlsx', engine='xlsxwriter') as writer: # Opens an Excel spreadsheet with the above name and...\n",
    "    type_token_df.to_excel(writer, sheet_name=f'type_token') # Writes the type-token dataframe to it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update log\n",
    "- Decided to add a date to each document to facilitate plotting by date (12.12.2023, 14:31)\n",
    "- Fixed an issue with files not being closed (12.12.2023, 18:06)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "- 'Passages' rather than 'documents' or 'texts' in variable names is a holdover from an earlier version, which I have allowed to remain as is"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
