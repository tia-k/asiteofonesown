<!DOCTYPE html>
<html>
    <head>
        <title>A (Short) Study in Sherlock - The Code</title>
        <link rel="stylesheet" type="text/css" href="stylespython.css" />
        <link rel="icon" type="image/x-icon" href="icons8-cube-24.png" />
    </head>
    <body>
        <div class="container">
            <div class="top">
                <h1>a (short) study in sherlock</h1>
                <p>A little website, for the purpose of a concise exercise in the digital humanities.</p>
                <p><a href="./">[index]</a> <a href="./methods">[methodology and ethos]</a> <a href="./python"><b>[python code navigation]</b></a></p>
            </div>
            <div class="main">
                <div class="essay">
                    <div class="codeblock">
                        <h2>Type-Token Ratio</h2>
                        <p>The full code can be found <a href="https://github.com/tia-k/asiteofonesown/blob/af3e0a50129b996f49e9230180728f424ecd555b/code/01A_type_token_analysis.ipynb" target="_blank">here</a> and <a href="https://github.com/tia-k/asiteofonesown/blob/af3e0a50129b996f49e9230180728f424ecd555b/code/01B_type_token_analysis.ipynb" target="_blank">here</a>.</p>
                        <p>Here, I created a function called <span class="inline-code">type_token()</span> to retrieve type-token data for each text. The code runs through each file in the Sherlock Holmes corpus, splits it into lowercase words without punctuation, and counts how long this list of words is to get the number of words (tokens). Following this, it removes stop words from the list and then goes through each word and adds it to a list of unique words if it is not already in that list; the length of that list is the number of types. This information, the TTR calculated from it, and the document's name and date as a number are then recorded to a dictionary, which the function returns. The dictionary for each document in the corpus is written to a list, which is then turned into a data frame (table) before being exported to a .xlsx file, which I manually edited in Excel to improve readability and produce visualisations. </p>
                        <p>One of the core issues I had while developing this code was getting the date into a format Excel could handle. I found that Excel could not handle dates before 1900 well, so I had to settle for presenting dates as numbers, with decimal values representing progression into each year so I could represent the difference between a text published in January and a text published in January. The other troubling element was found while writing this explanation; I failed to run the <span class="inline-code">clean_words()</span> function and thus make text lowercase and remove punctuation. Knowing this could lead to false counts of words being different from each other, I went back into the code to fix this. </p>
                        <p>Modules used: <span class="inline-code">glob, pathlib, <a href="https://pandas.pydata.org/" target="_blank">pandas</a>, string, <a href="https://xlsxwriter.readthedocs.io/">xlsxwriter</a></span></p>
                        <pre><code># Creating a function to do the type-token analysis for each text
def type_token(filepath, title):
    with open(f'{filepath}', encoding='utf8') as f: # Opens the .txt file as variable f
        full_passage = f.read().lower() # Reads the .txt file into a string and then closes the file
    full_passage = full_passage.replace('\n', ' ') # Replaces new-line symbols with spaces
    full_passage = full_passage.split() # Splits this string into a list of words
    for word in full_passage: # For each word in the list of words...
        word = clean_words(word) # This removes any remaining punctuation except for hyphens and makes it lowercase
    tokens = len(full_passage) # Counts the amount of words in the list of all the document's words - this is the number of 'tokens' in the document
    passage = [word for word in full_passage if word not in stopwords] # Makes a list of words with the stopwords removed (adds all the passage's words to a new list unless they are stopwords)

    unique_terms = [passage[0]] # Creates a list of unique terms, starting with the passage's first non-stopword word
    for word in passage: # For each word in the passage...
        if word not in unique_terms: # If it hasn't already been recorded as a unique word...
            unique_terms.append(word) # This adds the word to the list of unique words
    types = len(unique_terms) # Counts the amount of words in the list of the document's unique words - this is the number of 'types'
    ttr = round(types/tokens, 5) # Calculates the type-token ratio (number of types divided by number of tokens) and rounds this to five decimal places
    passage_date = title[0:4] # Creates a value for the document's year (the first four characters of the file's name)
    passage_month = title[5:7] # Creates a value for the document's month (the fifth and sixth characters of the file's name)
    if passage_month[0] == '0': # If the document's month has a trailing zero
        passage_month.replace('0', '') # This removes it
    passage_month_n = int(passage_month) # And this converts it to an integer to do some maths
    passage_month_n = passage_month_n - 1 # And this subtracts one for the purpose of some maths

    # The following maths works by estimating one month into the year being 0.08333, so, if the year is 2023, January is 2023.000000 (so the month needs 1 subtracted), February is 2023.08333 and so on; the purpose of all this is to make visualisations in Excel easier
    passage_month_calc = round(passage_month_n * 0.08333, 5) # Multiplies the month by 0.08333
    passage_month = str(passage_month_calc) # Converts this float value back to a string
    passage_month = passage_month[1:] # Removes the starting zero
    passage_date = passage_date + passage_month # Adds this decimal onto the end of the year string
    document_type_token = {'text': title, 'date': passage_date, 'TTR': ttr, 'types': types,'tokens': tokens} # Creates a dictionary containing the document's title, number of types, number of tokens, and type-token ratio (TTR)
    return document_type_token # Returns this list to be used by the wider loop</code></pre>
                    </div>
                    <div class="codeblock">
                        <h2>Sentiment Analysis</h2>
                        <p>The full code can be found <a href="https://github.com/tia-k/asiteofonesown/blob/af3e0a50129b996f49e9230180728f424ecd555b/code/02_sentiment_analysis.ipynb" target="_blank">here</a>.</p>
                        <p>In this code, I used the <a href="https://github.com/cjhutto/vaderSentiment" target="_blank"><span class="inline-code">vaderSentiment</span></a> module to perform the sentiment analysis (a more detailed explanation of how it was used can be viewed on the <a href="./methods">methodology</a> page of this site) and the <a href="https://www.nltk.org/" target="_blank"><span class="inline-code">nltk</span></a> (Natural Language Toolkit) module for splitting texts into sentences. The code uses the <span class="inline-code">find_sentiment()</span> function I created to split the text of each document into sentences, work out the overall sentiment for each of these sentences, and then calculate the average sentiment for each five per cent of the document, equated to five per cent of the total number of sentences. This data for each document is returned as a dictionary and added to a variable containing the sentiment data for all documents. This variable is then converted to a data frame (table) and exported to a .xlsx file. I then manually formatted data in Excel for more readability and to create visualisations.</p>
                        <p>Modules used: <span class="inline-code"><a href="https://github.com/cjhutto/vaderSentiment" target="_blank">vaderSentiment</a>, glob, pathlib, <a href="https://www.nltk.org/" target="_blank">nltk</a>, <a href="https://pandas.pydata.org/" target="_blank">pandas</a>, statistics, <a href="https://xlsxwriter.readthedocs.io/">xlsxwriter</a></span></p>
                        <pre>
                            <code># Creating a function to do the sentiment analysis for each 5% of the text
def find_sentiment(filepath,title):
    with open(f'{filepath}', encoding='utf8') as f: # Opens the .txt file as variable f
        text = f.read() # Reads the .txt file into a string and then closes the file
    text = text.replace('\n', ' ') # Replaces new-line symbols with spaces
    text_sentences = nltk.sent_tokenize(text) # Splits the text up into a list of sentences using a NLTK (Natural Language Toolkit) method
    
    no_sentences = len(text_sentences) # Finds the number of sentences by getting the length of the list of sentences
    start_pos = 0 # Creates a starting position for sentence number, beginning at zero (counting starts at zero in Python)
    five_percent = round(no_sentences/20) # Calculates how many sentences are equivalent to five percent of the whole document

    section_scores = {'text':f'{title}',} # Creates an empty dictionary which will go on to contain the sentiment score for each five percent, and records the document's name as its first key/value pair

    for i in range(1, 21): # For each five percent...
        total_scores = [] # This makes an empty list which will store the scores for each sentence
        for sentence in text_sentences[start_pos:start_pos+five_percent]: # For every sentence in this five percent...
            scores = sentimentAnalyser.polarity_scores(sentence) # This works out its scores
            total_scores.append(scores['compound']) #This adds its overall score to the list of scores
        
        if len(total_scores) != 0: # If there are sentences in the list of scores for this five percent...
            scores_count = mean(total_scores) # This works out the average of this scores and stores it
        else: # If there aren't...
            scores_count = 0 # This sets the overall score for that section to zero

        section_scores[i*5] = float(scores_count) # Makes a new key/value pair in the dictionary, with the key of which percent it is, and the average sentiment value
        start_pos += five_percent # Increases the starting position to after this five percent, so we work with the next five percent of the document

    return section_scores # Returns the dictionary of a document's sentiment scores</code></pre>
                    </div>
                    <div class="codeblock">
                        <h2>Natural Language Processing</h2>
                        <p>The full code can be found <a href="https://github.com/tia-k/asiteofonesown/blob/af3e0a50129b996f49e9230180728f424ecd555b/code/03_natural_language_processing.ipynb" target="_blank">here</a>.</p>
                        <p>For this code, I used <a href="https://spacy.io/" target="_blank"><span class="inline-code">spacy</span></a> for natural language processing. The <span class="inline-code">top_speech_parts()</span> function I developed was used to read text from each document in a corpus and process it. spaCy tokenises the text and recognises which part of speech each word was. A loop then runs through each word and stores adjectives, nouns, pronouns and verbs in their respective lists. The counter function is then used to count the number of uses for each word of each type, the <span class="inline-code">.most_common()</span> method sorts each list, and the function returns the data for each document. A .xlsx file is then created for each document based on its data. To simplify the code, I elected to have one Excel file for each document and collated the data manually.</p>
                        <p>Within the <span class="inline-code">top_speech_parts()</span> function is a reference to a function called <span class="inline-code">make_tokens()</span>; this function was one I created for a different attempt at natural language processing, the data I have not used. Consequently, I will not explain it in depth here, but its logic was to split the text into lists of words and labels, divide it into a list of bigrams (sets of two consecutive words), and then search through these bigrams to find words around specified keywords. This data was also written to data frames and exported to Excel files for each document.</p>
                        <p>Modules used: <span class="inline-code"><a href="https://spacy.io/" target="_blank">spacy</a>, collections, <a href="https://pandas.pydata.org/" target="_blank">panda</a>, glob, pathlib</span></p>
                        <pre>
                            <code># Creating a function for working out the most common adjectives, nouns, pronouns and verbs in each text
def top_speech_parts(filepath):
    with open(filepath, encoding="utf-8") as f: # Opens the .txt file (document) as variable f
        document = nlp(f.read().lower()) # Reads the .txt file into the 'document' variable, applies NLP to it and then closes the file
    adjs = [] # Creates an empty list that will store the adjective data
    nouns = [] # Creates an empty list that will store the noun data
    pronouns = [] # Creates an empty list that will store the pronoun data
    verbs = [] # Creates an empty list that will store the verb data
    
    for token in document: # For every word in the document...
        if token.pos_ == 'ADJ': # If it is an adjective...
            adjs.append(token.text) # Record it to the adjective list
        elif token.pos_ == 'NOUN': # If it is a noun...
            nouns.append(token.text) # Record it to the noun list
        elif token.pos_ == 'PRON': # If it is a pronoun...
            pronouns.append(token.text) # Record it to the pronoun list
        elif token.pos_ == 'VERB': # If it is a verb...
            verbs.append(token.text) # Record it to the verb list
        else: # If it is not any of these...
            pass # Go on to the next word
    
    tokens = make_tokens(document) # Stores the value of what the make_tokens function created below returns

    adjs_tally = Counter(adjs) # Counts how many times each adjective is used in a tuple with the word and its number of incidences
    adjs_tally = adjs_tally.most_common() # Reorders this data from most common to least common

    nouns_tally = Counter(nouns) # Counts how many times each noun is used in a tuple with the word and its number of incidences
    nouns_tally = nouns_tally.most_common() # Reorders this data from most common to least common

    pronouns_tally = Counter(pronouns) # Counts how many times each pronoun is used in a tuple with the word and its number of incidences
    pronouns_tally = pronouns_tally.most_common() # Reorders this data from most common to least common

    verbs_tally = Counter(verbs) # Counts how many times each verb is used in a tuple with the word and its number of incidences
    verbs_tally = verbs_tally.most_common() # Reorders this data from most common to least common
    
    return adjs_tally, nouns_tally, pronouns_tally, verbs_tally, tokens # Returns the tallies of adjectives, nouns, pronouns and verbs, and the list of tokens in the text and their part-of-speech</code></pre>
                    </div>
                </div>
                <div class="footnotes">
                    <ul>
                        <li><a target="_blank" href="https://icons8.com/icon/98119/cube">Cube</a> icon by <a target="_blank" href="https://icons8.com">Icons8</a></li>
                    </ul>
                </div>
            </div>
        </div>
    </body>
</html>
